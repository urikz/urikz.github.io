---
---

@misc{dejong2021mention,
      title={Mention Memory: incorporating textual knowledge into Transformers through entity mention attention}, 
      author={Michiel de Jong* and Yury Zemlyanskiy* and Nicholas FitzGerald and Fei Sha and William Cohen},
      year={2021},
      eprint={2110.06176},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      html      = {https://arxiv.org/abs/2110.06176},
      pdf       = {https://arxiv.org/pdf/2110.06176.pdf},      
      abstract={Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with `mention memory', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining.},
      selected  = {true},
      code = {https://github.com/google-research/language/commits/master/language/mentionmemory}

}

@inproceedings{DBLP:conf/naacl/ZemlyanskiyAJPE21,
  author    = {Yury Zemlyanskiy and
               Joshua Ainslie and
               Michiel de Jong and
               Philip Pham and
               Ilya Eckstein and
               Fei Sha},
  editor    = {Kristina Toutanova and
               Anna Rumshisky and
               Luke Zettlemoyer and
               Dilek Hakkani{-}T{\"{u}}r and
               Iz Beltagy and
               Steven Bethard and
               Ryan Cotterell and
               Tanmoy Chakraborty and
               Yichao Zhou},
  title     = {ReadTwice: Reading Very Large Documents with Memories},
  booktitle = {{NAACL-HLT}},
  pages     = {5189--5195},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.naacl-main.408},
  doi       = {10.18653/v1/2021.naacl-main.408},
  timestamp = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/ZemlyanskiyAJPE21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  html      = {https://arxiv.org/abs/2105.04241},
  pdf       = {https://arxiv.org/pdf/2105.04241},
  abstract  = {Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose ReadTwice, a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books. Source code and pre-trained checkpoints for ReadTwice can be found at https://goo.gle/research-readtwice.},
  code      = {https://github.com/google-research/google-research/tree/master/readtwice},
  selected  = {true}    
}

@inproceedings{DBLP:conf/eacl/ZemlyanskiyGHKR21,
  author    = {Yury Zemlyanskiy and
               Sudeep Gandhe and
               Ruining He and
               Bhargav Kanagal and
               Anirudh Ravula and
               Juraj Gottweis and
               Fei Sha and
               Ilya Eckstein},
  editor    = {Paola Merlo and
               J{\"{o}}rg Tiedemann and
               Reut Tsarfaty},
  title     = {{DOCENT:} Learning Self-Supervised Entity Representations from Large
               Document Collections},
  booktitle = {{EACL}},
  pages     = {2540--2549},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://aclanthology.org/2021.eacl-main.217/},
  timestamp = {Fri, 06 Aug 2021 00:40:45 +0200},
  biburl    = {https://dblp.org/rec/conf/eacl/ZemlyanskiyGHKR21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  html      = {https://arxiv.org/abs/2102.13247},
  pdf       = {https://arxiv.org/pdf/2102.13247},
  abstract={This paper explores learning rich self-supervised entity representations from large amounts of the associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, question answering, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of context to include any available text related to an entity. This enables a new class of powerful, high-capacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision.
We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities -- strategies we compare experimentally on downstream tasks in the TV-Movies domain, such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results, our models match or outperform competitive baselines, sometimes with little or no fine-tuning, and can scale to very large corpora.
Finally, we make our datasets and pre-trained models publicly available. This includes Reviews2Movielens (see https://goo.gle/research-docent ), mapping the up to 1B word corpus of Amazon movie reviews (He and McAuley, 2016) to MovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions (see this https URL ) with natural language queries and corresponding community recommendations.},  
  code      = {https://github.com/google-research/google-research/tree/master/docent},
  selected  = {true}  
}

@inproceedings{DBLP:conf/acl/RuffZVSK19,
  author    = {Lukas Ruff and
               Yury Zemlyanskiy and
               Robert A. Vandermeulen and
               Thomas Schnake and
               Marius Kloft},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {Self-Attentive, Multi-Context One-Class Classification for Unsupervised
               Anomaly Detection on Text},
  booktitle = {{ACL}},
  pages     = {4061--4071},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1398},
  doi       = {10.18653/v1/p19-1398},
  timestamp = {Fri, 06 Aug 2021 00:41:01 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/RuffZVSK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  html      = {https://aclanthology.org/P19-1398/},
  pdf       = {https://aclanthology.org/P19-1398.pdf},
  abstract  = {There exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. In this paper we introduce a new anomaly detection method—Context Vector Data Description (CVDD)—which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus. These contexts in combination with the self-attention weights make our method highly interpretable. We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the well-known Reuters, 20 Newsgroups, and IMDB Movie Reviews datasets.},
  code      = {https://github.com/lukasruff/CVDD-PyTorch}
}

@inproceedings{DBLP:conf/conll/ZemlyanskiyS18,
  author    = {Yury Zemlyanskiy and
               Fei Sha},
  editor    = {Anna Korhonen and
               Ivan Titov},
  title     = {Aiming to Know You Better Perhaps Makes Me a More Engaging Dialogue
               Partner},
  booktitle = {CoNLL},
  pages     = {551--561},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/k18-1053},
  doi       = {10.18653/v1/k18-1053},
  timestamp = {Fri, 06 Aug 2021 00:41:09 +0200},
  biburl    = {https://dblp.org/rec/conf/conll/ZemlyanskiyS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  html      = {https://arxiv.org/abs/1808.07104},
  pdf       = {https://arxiv.org/pdf/1808.07104.pdf},
  abstract  = {There have been several attempts to define a plausible motivation for a chit-chat dialogue agent that can lead to engaging conversations. In this work, we explore a new direction where the agent specifically focuses on discovering information about its interlocutor. We formalize this approach by defining a quantitative metric. We propose an algorithm for the agent to maximize it. We validate the idea with human evaluation where our system outperforms various baselines. We demonstrate that the metric indeed correlates with the human judgments of engagingness.},
  code = {https://github.com/urikz/ChatBot},
  selected  = {true}  
}

@inproceedings{eck2014extracting,
  title={Extracting translation pairs from social network content},
  author={Eck, Matthias and Zemlyanskiy, Yury and Zhang, Joy and Waibel, Alex},
  booktitle = {IWSLT},  
  year={2014},
  html={https://research.fb.com/publications/extracting-translation-pairs-from-social-network-content/},
  pdf={https://research.fb.com/wp-content/uploads/2016/11/publication00122_download0001.pdf},
  abstract={We introduce two methods to collect additional training data for statistical machine translation systems from public social network content. The first method identifies multilingual content where the author self-translated their own post to reach additional friends, fans or customers. Once identified, we can split the post in the language segments and extract translation pairs from this content. The second methods considers web links (URLs) that users add as part of their post to point the reader to a video, article or website. If the same URL is shared from different language users, there is a chance they might give the same comment in their respective language. We use a support vector machine (SVM) as a classifier to identify true translations from all candidate pairs. We collected additional translation pairs using both methods for the language pairs Spanish-English and Portuguese-English. Testing the collected data as additional training data for statistical machine translations on in-domain test sets resulted in very significant improvements of up to 5 BLEU.}
}